Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1039353: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Mon Apr  3 12:44:45 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Mon Apr  3 12:57:44 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Mon Apr  3 12:57:44 2023
Terminated at Mon Apr  3 13:09:47 2023
Results reported at Mon Apr  3 13:09:47 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2609.43 sec.
    Max Memory :                                 3930 MB
    Average Memory :                             618.57 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                69
    Run time :                                   722 sec.
    Turnaround time :                            1502 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 345.25702 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 176.78533 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 90.89411 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 49.70439 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 28.65023 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 18.49446 seconds.
---------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1039382: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Mon Apr  3 17:31:15 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Mon Apr  3 17:44:14 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Mon Apr  3 17:44:14 2023
Terminated at Mon Apr  3 17:56:16 2023
Results reported at Mon Apr  3 17:56:16 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2560.67 sec.
    Max Memory :                                 3862 MB
    Average Memory :                             552.79 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                89
    Run time :                                   722 sec.
    Turnaround time :                            1501 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 343.98082 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 176.89689 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 90.88494 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 49.82296 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 29.73601 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: SCIPY_DIRECTED_HAUSDORFF
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 19.14909 seconds.
---------------------------------------------------
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 40 slots
that were requested by the application:
  python

Either request fewer slots for your application, or make more slots available
for use.
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1039452: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Mon Apr  3 23:43:40 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Tue Apr  4 04:44:12 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Tue Apr  4 04:44:12 2023
Terminated at Tue Apr  4 08:19:10 2023
Results reported at Tue Apr  4 08:19:10 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   40290.00 sec.
    Max Memory :                                 4597 MB
    Average Memory :                             675.05 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2 MB
    Max Processes :                              37
    Max Threads :                                109
    Run time :                                   12897 sec.
    Turnaround time :                            30930 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: EARLYBREAK
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 5338.95329 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: EARLYBREAK
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 3109.38236 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: EARLYBREAK
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 1847.89368 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: EARLYBREAK
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 1348.31174 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: EARLYBREAK
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 734.29325 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: EARLYBREAK
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 500.23203 seconds.
---------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1039510: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Tue Apr  4 10:44:00 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Tue Apr  4 10:44:13 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Tue Apr  4 10:44:13 2023
Terminated at Tue Apr  4 10:47:53 2023
Results reported at Tue Apr  4 10:47:53 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   960.00 sec.
    Max Memory :                                 2045 MB
    Average Memory :                             385.66 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                1609
    Run time :                                   220 sec.
    Turnaround time :                            233 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[9279,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[9028,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[8809,1],1]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[8215,1],7]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[7451,1],7]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[5918,1],20]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1039763: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Wed Apr  5 01:05:30 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Wed Apr  5 13:10:22 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Wed Apr  5 13:10:22 2023
Terminated at Wed Apr  5 15:36:05 2023
Results reported at Wed Apr  5 15:36:05 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32723.00 sec.
    Max Memory :                                 4249 MB
    Average Memory :                             532.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2 MB
    Max Processes :                              38
    Max Threads :                                653
    Run time :                                   8742 sec.
    Turnaround time :                            52235 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[13966,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[10179,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[6170,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[2088,1],7]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[555,1],14]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[31028,1],3]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1040311: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Wed Apr  5 17:05:39 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Wed Apr  5 23:36:09 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Wed Apr  5 23:36:09 2023
Terminated at Wed Apr  5 23:36:21 2023
Results reported at Wed Apr  5 23:36:21 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   259.00 sec.
    Max Memory :                                 71 MB
    Average Memory :                             15.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                89
    Run time :                                   13 sec.
    Turnaround time :                            23442 sec.

The output (if any) follows:

-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[11879,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[11718,1],1]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[11646,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[11228,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[10385,1],7]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[9215,1],1]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1040540: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Thu Apr  6 10:44:45 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Thu Apr  6 10:44:45 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Thu Apr  6 10:44:45 2023
Terminated at Thu Apr  6 10:47:04 2023
Results reported at Thu Apr  6 10:47:04 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   141.00 sec.
    Max Memory :                                 161 MB
    Average Memory :                             144.65 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                89
    Run time :                                   137 sec.
    Turnaround time :                            139 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1040544: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Thu Apr  6 10:47:45 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Thu Apr  6 12:49:05 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Thu Apr  6 12:49:05 2023
Terminated at Fri Apr  7 00:09:32 2023
Results reported at Fri Apr  7 00:09:32 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   132338.00 sec.
    Max Memory :                                 4821 MB
    Average Memory :                             588.08 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              37
    Max Threads :                                109
    Run time :                                   40832 sec.
    Turnaround time :                            48107 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 19358.79177 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 10168.06969 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 5269.34272 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 3052.92771 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 1859.60530 seconds.
---------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: KDTREE
Total model count: 1000
--------------------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 1106.04466 seconds.
---------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044835: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:15:25 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:15:26 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:15:26 2023
Terminated at Fri Apr 21 14:15:55 2023
Results reported at Fri Apr 21 14:15:55 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   356.00 sec.
    Max Memory :                                 261 MB
    Average Memory :                             65.67 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                89
    Run time :                                   29 sec.
    Turnaround time :                            30 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[58749,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[58613,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[58301,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[57879,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[61308,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[59770,1],0]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044838: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:20:33 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:21:36 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:21:36 2023
Terminated at Fri Apr 21 14:21:56 2023
Results reported at Fri Apr 21 14:21:56 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   351.00 sec.
    Max Memory :                                 1393 MB
    Average Memory :                             838.20 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                221
    Run time :                                   21 sec.
    Turnaround time :                            83 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[29173,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[29019,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28716,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28343,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[27574,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[26028,1],0]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044839: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:33:38 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:33:39 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:33:39 2023
Terminated at Fri Apr 21 14:33:58 2023
Results reported at Fri Apr 21 14:33:58 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   339.00 sec.
    Max Memory :                                 2207 MB
    Average Memory :                             1328.40 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              23
    Max Threads :                                89
    Run time :                                   19 sec.
    Turnaround time :                            20 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[55975,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[55858,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[55802,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[55422,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[41438,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[43970,1],0]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044840: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:34:42 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:34:42 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:34:42 2023
Terminated at Fri Apr 21 14:34:49 2023
Results reported at Fri Apr 21 14:34:49 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   3.81 sec.
    Max Memory :                                 21 MB
    Average Memory :                             14.33 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   9 sec.
    Turnaround time :                            7 sec.

The output (if any) follows:

-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21842,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21863,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21868,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21874,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21637,1],0]
  Exit code:    1
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[21663,1],0]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044841: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Exited

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:35:53 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:35:53 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:35:53 2023
Terminated at Fri Apr 21 14:36:11 2023
Results reported at Fri Apr 21 14:36:11 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   323.00 sec.
    Max Memory :                                 2207 MB
    Average Memory :                             1327.80 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              23
    Max Threads :                                89
    Run time :                                   21 sec.
    Turnaround time :                            18 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
['bed_0608', 'bathtub_0146', 'tv_stand_0306', 'airplane_0642', 'bathtub_0143', 'bottle_0403', 'bottle_0425', 'vase_0495', 'xbox_0112', 'door_0127', 'vase_0551', 'bottle_0362', 'bathtub_0141', 'bed_0527', 'plant_0307', 'bed_0517', 'radio_0118', 'car_0273', 'vase_0547', 'guitar_0192', 'bed_0603', 'airplane_0673', 'bed_0554', 'car_0269', 'plant_0268', 'vase_0523', 'plant_0266', 'bottle_0389', 'tent_0182', 'bed_0591', 'guitar_0218', 'bed_0522', 'bed_0611', 'bed_0613', 'bed_0587', 'guitar_0220', 'bathtub_0150', 'vase_0507', 'guitar_0239', 'bed_0555', 'vase_0562', 'bottle_0377', 'airplane_0667', 'bed_0595', 'plant_0288', 'tv_stand_0361', 'car_0258', 'bed_0590', 'car_0272', 'car_0242', 'airplane_0648', 'airplane_0639', 'car_0287', 'bed_0600', 'guitar_0191', 'guitar_0168', 'plant_0270', 'guitar_0224', 'vase_0532', 'tv_stand_0364', 'wardrobe_0100', 'bed_0586', 'tv_stand_0275', 'xbox_0117', 'plant_0281', 'vase_0477', 'airplane_0692', 'radio_0105', 'laptop_0151', 'vase_0544', 'guitar_0235', 'guitar_0189', 'wardrobe_0107', 'vase_0556', 'tv_stand_0333', 'plant_0289', 'person_0102', 'car_0205', 'airplane_0705', 'tv_stand_0297', 'plant_0252', 'car_0276', 'vase_0511', 'xbox_0105', 'laptop_0159', 'car_0225', 'bed_0561', 'tent_0164', 'airplane_0709', 'door_0118', 'airplane_0647', 'tv_stand_0320', 'airplane_0677', 'plant_0259', 'bench_0186', 'car_0256', 'bathtub_0138', 'bottle_0380', 'vase_0513', 'car_0207']
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[45892,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
['bed_0608', 'bathtub_0146', 'tv_stand_0306', 'airplane_0642', 'bathtub_0143', 'bottle_0403', 'bottle_0425', 'vase_0495', 'xbox_0112', 'door_0127', 'vase_0551', 'bottle_0362', 'bathtub_0141', 'bed_0527', 'plant_0307', 'bed_0517', 'radio_0118', 'car_0273', 'vase_0547', 'guitar_0192', 'bed_0603', 'airplane_0673', 'bed_0554', 'car_0269', 'plant_0268', 'vase_0523', 'plant_0266', 'bottle_0389', 'tent_0182', 'bed_0591', 'guitar_0218', 'bed_0522', 'bed_0611', 'bed_0613', 'bed_0587', 'guitar_0220', 'bathtub_0150', 'vase_0507', 'guitar_0239', 'bed_0555', 'vase_0562', 'bottle_0377', 'airplane_0667', 'bed_0595', 'plant_0288', 'tv_stand_0361', 'car_0258', 'bed_0590', 'car_0272', 'car_0242', 'airplane_0648', 'airplane_0639', 'car_0287', 'bed_0600', 'guitar_0191', 'guitar_0168', 'plant_0270', 'guitar_0224', 'vase_0532', 'tv_stand_0364', 'wardrobe_0100', 'bed_0586', 'tv_stand_0275', 'xbox_0117', 'plant_0281', 'vase_0477', 'airplane_0692', 'radio_0105', 'laptop_0151', 'vase_0544', 'guitar_0235', 'guitar_0189', 'wardrobe_0107', 'vase_0556', 'tv_stand_0333', 'plant_0289', 'person_0102', 'car_0205', 'airplane_0705', 'tv_stand_0297', 'plant_0252', 'car_0276', 'vase_0511', 'xbox_0105', 'laptop_0159', 'car_0225', 'bed_0561', 'tent_0164', 'airplane_0709', 'door_0118', 'airplane_0647', 'tv_stand_0320', 'airplane_0677', 'plant_0259', 'bench_0186', 'car_0256', 'bathtub_0138', 'bottle_0380', 'vase_0513', 'car_0207']
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[45805,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
['bed_0608', 'bathtub_0146', 'tv_stand_0306', 'airplane_0642', 'bathtub_0143', 'bottle_0403', 'bottle_0425', 'vase_0495', 'xbox_0112', 'door_0127', 'vase_0551', 'bottle_0362', 'bathtub_0141', 'bed_0527', 'plant_0307', 'bed_0517', 'radio_0118', 'car_0273', 'vase_0547', 'guitar_0192', 'bed_0603', 'airplane_0673', 'bed_0554', 'car_0269', 'plant_0268', 'vase_0523', 'plant_0266', 'bottle_0389', 'tent_0182', 'bed_0591', 'guitar_0218', 'bed_0522', 'bed_0611', 'bed_0613', 'bed_0587', 'guitar_0220', 'bathtub_0150', 'vase_0507', 'guitar_0239', 'bed_0555', 'vase_0562', 'bottle_0377', 'airplane_0667', 'bed_0595', 'plant_0288', 'tv_stand_0361', 'car_0258', 'bed_0590', 'car_0272', 'car_0242', 'airplane_0648', 'airplane_0639', 'car_0287', 'bed_0600', 'guitar_0191', 'guitar_0168', 'plant_0270', 'guitar_0224', 'vase_0532', 'tv_stand_0364', 'wardrobe_0100', 'bed_0586', 'tv_stand_0275', 'xbox_0117', 'plant_0281', 'vase_0477', 'airplane_0692', 'radio_0105', 'laptop_0151', 'vase_0544', 'guitar_0235', 'guitar_0189', 'wardrobe_0107', 'vase_0556', 'tv_stand_0333', 'plant_0289', 'person_0102', 'car_0205', 'airplane_0705', 'tv_stand_0297', 'plant_0252', 'car_0276', 'vase_0511', 'xbox_0105', 'laptop_0159', 'car_0225', 'bed_0561', 'tent_0164', 'airplane_0709', 'door_0118', 'airplane_0647', 'tv_stand_0320', 'airplane_0677', 'plant_0259', 'bench_0186', 'car_0256', 'bathtub_0138', 'bottle_0380', 'vase_0513', 'car_0207']
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[45460,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
['bed_0608', 'bathtub_0146', 'tv_stand_0306', 'airplane_0642', 'bathtub_0143', 'bottle_0403', 'bottle_0425', 'vase_0495', 'xbox_0112', 'door_0127', 'vase_0551', 'bottle_0362', 'bathtub_0141', 'bed_0527', 'plant_0307', 'bed_0517', 'radio_0118', 'car_0273', 'vase_0547', 'guitar_0192', 'bed_0603', 'airplane_0673', 'bed_0554', 'car_0269', 'plant_0268', 'vase_0523', 'plant_0266', 'bottle_0389', 'tent_0182', 'bed_0591', 'guitar_0218', 'bed_0522', 'bed_0611', 'bed_0613', 'bed_0587', 'guitar_0220', 'bathtub_0150', 'vase_0507', 'guitar_0239', 'bed_0555', 'vase_0562', 'bottle_0377', 'airplane_0667', 'bed_0595', 'plant_0288', 'tv_stand_0361', 'car_0258', 'bed_0590', 'car_0272', 'car_0242', 'airplane_0648', 'airplane_0639', 'car_0287', 'bed_0600', 'guitar_0191', 'guitar_0168', 'plant_0270', 'guitar_0224', 'vase_0532', 'tv_stand_0364', 'wardrobe_0100', 'bed_0586', 'tv_stand_0275', 'xbox_0117', 'plant_0281', 'vase_0477', 'airplane_0692', 'radio_0105', 'laptop_0151', 'vase_0544', 'guitar_0235', 'guitar_0189', 'wardrobe_0107', 'vase_0556', 'tv_stand_0333', 'plant_0289', 'person_0102', 'car_0205', 'airplane_0705', 'tv_stand_0297', 'plant_0252', 'car_0276', 'vase_0511', 'xbox_0105', 'laptop_0159', 'car_0225', 'bed_0561', 'tent_0164', 'airplane_0709', 'door_0118', 'airplane_0647', 'tv_stand_0320', 'airplane_0677', 'plant_0259', 'bench_0186', 'car_0256', 'bathtub_0138', 'bottle_0380', 'vase_0513', 'car_0207']
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[45084,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
['bed_0608', 'bathtub_0146', 'tv_stand_0306', 'airplane_0642', 'bathtub_0143', 'bottle_0403', 'bottle_0425', 'vase_0495', 'xbox_0112', 'door_0127', 'vase_0551', 'bottle_0362', 'bathtub_0141', 'bed_0527', 'plant_0307', 'bed_0517', 'radio_0118', 'car_0273', 'vase_0547', 'guitar_0192', 'bed_0603', 'airplane_0673', 'bed_0554', 'car_0269', 'plant_0268', 'vase_0523', 'plant_0266', 'bottle_0389', 'tent_0182', 'bed_0591', 'guitar_0218', 'bed_0522', 'bed_0611', 'bed_0613', 'bed_0587', 'guitar_0220', 'bathtub_0150', 'vase_0507', 'guitar_0239', 'bed_0555', 'vase_0562', 'bottle_0377', 'airplane_0667', 'bed_0595', 'plant_0288', 'tv_stand_0361', 'car_0258', 'bed_0590', 'car_0272', 'car_0242', 'airplane_0648', 'airplane_0639', 'car_0287', 'bed_0600', 'guitar_0191', 'guitar_0168', 'plant_0270', 'guitar_0224', 'vase_0532', 'tv_stand_0364', 'wardrobe_0100', 'bed_0586', 'tv_stand_0275', 'xbox_0117', 'plant_0281', 'vase_0477', 'airplane_0692', 'radio_0105', 'laptop_0151', 'vase_0544', 'guitar_0235', 'guitar_0189', 'wardrobe_0107', 'vase_0556', 'tv_stand_0333', 'plant_0289', 'person_0102', 'car_0205', 'airplane_0705', 'tv_stand_0297', 'plant_0252', 'car_0276', 'vase_0511', 'xbox_0105', 'laptop_0159', 'car_0225', 'bed_0561', 'tent_0164', 'airplane_0709', 'door_0118', 'airplane_0647', 'tv_stand_0320', 'airplane_0677', 'plant_0259', 'bench_0186', 'car_0256', 'bathtub_0138', 'bottle_0380', 'vase_0513', 'car_0207']
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[48391,1],0]
  Exit code:    1
--------------------------------------------------------------------------
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
['bed_0608', 'bathtub_0146', 'tv_stand_0306', 'airplane_0642', 'bathtub_0143', 'bottle_0403', 'bottle_0425', 'vase_0495', 'xbox_0112', 'door_0127', 'vase_0551', 'bottle_0362', 'bathtub_0141', 'bed_0527', 'plant_0307', 'bed_0517', 'radio_0118', 'car_0273', 'vase_0547', 'guitar_0192', 'bed_0603', 'airplane_0673', 'bed_0554', 'car_0269', 'plant_0268', 'vase_0523', 'plant_0266', 'bottle_0389', 'tent_0182', 'bed_0591', 'guitar_0218', 'bed_0522', 'bed_0611', 'bed_0613', 'bed_0587', 'guitar_0220', 'bathtub_0150', 'vase_0507', 'guitar_0239', 'bed_0555', 'vase_0562', 'bottle_0377', 'airplane_0667', 'bed_0595', 'plant_0288', 'tv_stand_0361', 'car_0258', 'bed_0590', 'car_0272', 'car_0242', 'airplane_0648', 'airplane_0639', 'car_0287', 'bed_0600', 'guitar_0191', 'guitar_0168', 'plant_0270', 'guitar_0224', 'vase_0532', 'tv_stand_0364', 'wardrobe_0100', 'bed_0586', 'tv_stand_0275', 'xbox_0117', 'plant_0281', 'vase_0477', 'airplane_0692', 'radio_0105', 'laptop_0151', 'vase_0544', 'guitar_0235', 'guitar_0189', 'wardrobe_0107', 'vase_0556', 'tv_stand_0333', 'plant_0289', 'person_0102', 'car_0205', 'airplane_0705', 'tv_stand_0297', 'plant_0252', 'car_0276', 'vase_0511', 'xbox_0105', 'laptop_0159', 'car_0225', 'bed_0561', 'tent_0164', 'airplane_0709', 'door_0118', 'airplane_0647', 'tv_stand_0320', 'airplane_0677', 'plant_0259', 'bench_0186', 'car_0256', 'bathtub_0138', 'bottle_0380', 'vase_0513', 'car_0207']
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[34459,1],0]
  Exit code:    1
--------------------------------------------------------------------------


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044842: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; #mpiexec -n 1 python distributed_search.py;#mpiexec -n 2 python distributed_search.py;#mpiexec -n 4 python distributed_search.py;#mpiexec -n 8 python distributed_search.py;#mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; #mpiexec -n 1 python distributed_search.py;#mpiexec -n 2 python distributed_search.py;#mpiexec -n 4 python distributed_search.py;#mpiexec -n 8 python distributed_search.py;#mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:38:25 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:38:26 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:38:26 2023
Terminated at Fri Apr 21 14:38:34 2023
Results reported at Fri Apr 21 14:38:34 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

#mpiexec -n 1 python distributed_search.py
#mpiexec -n 2 python distributed_search.py
#mpiexec -n 4 python distributed_search.py
#mpiexec -n 8 python distributed_search.py
#mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   198.01 sec.
    Max Memory :                                 205 MB
    Average Memory :                             137.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                819
    Run time :                                   15 sec.
    Turnaround time :                            9 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 4.80911 seconds.


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c4-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044843: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:46:01 2023
Job was executed on host(s) <20*polus-c4-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:46:01 2023
                            <12*polus-c3-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 14:46:01 2023
Terminated at Fri Apr 21 14:47:23 2023
Results reported at Fri Apr 21 14:47:23 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   498.42 sec.
    Max Memory :                                 2666 MB
    Average Memory :                             1031.29 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                89
    Run time :                                   82 sec.
    Turnaround time :                            82 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 29.19667 seconds.
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 17.16263 seconds.
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 9.43254 seconds.
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 6.69566 seconds.
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 4.93147 seconds.
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 100
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is guitar_0235
Parallel Search Time: 3.96578 seconds.


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044846: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 14:58:10 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 15:11:24 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 15:11:24 2023
Terminated at Fri Apr 21 15:20:06 2023
Results reported at Fri Apr 21 15:20:06 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1972.56 sec.
    Max Memory :                                 3817 MB
    Average Memory :                             723.18 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                89
    Run time :                                   523 sec.
    Turnaround time :                            1316 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 1000
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 246.41576 seconds.
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 1000
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 123.47629 seconds.
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 1000
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 64.53104 seconds.
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 1000
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 36.35672 seconds.
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 1000
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 22.85596 seconds.
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 1000
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0712
Parallel Search Time: 16.51895 seconds.


PS:

Read file <ds_result.err> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1044848: <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> in cluster <MSUCluster> Done

Job <module load OpenMPI/2.1.3;module load Anaconda3/2019.07;source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh;conda activate;#BSUB -n 32 -q extended;#BSUB -W 12:00;#BSUB  -o ds_result.out;#BSUB  -e ds_result.err; mpiexec -n 1 python distributed_search.py;mpiexec -n 2 python distributed_search.py;mpiexec -n 4 python distributed_search.py;mpiexec -n 8 python distributed_search.py;mpiexec -n 16 python distributed_search.py;mpiexec -n 32 python distributed_search.py;#mpiexec -n 40 python distributed_search.py;#mpiexec -n 64 python distributed_search.py> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 16:05:03 2023
Job was executed on host(s) <20*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <extended>, as user <edu-cmc-sqi21-19> in cluster <MSUCluster> at Fri Apr 21 18:12:42 2023
                            <12*polus-c4-ib.bmc.hpc.cs.msu.ru>
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19> was used as the home directory.
</home_edu/edu-cmc-sqi21/edu-cmc-sqi21-19/HD> was used as the working directory.
Started at Fri Apr 21 18:12:42 2023
Terminated at Fri Apr 21 19:38:43 2023
Results reported at Fri Apr 21 19:38:43 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
module load OpenMPI/2.1.3
module load Anaconda3/2019.07
source /polusfs/modules/anaconda/anaconda-3/etc/profile.d/conda.sh
conda activate
#BSUB -n 32 -q extended
#BSUB -W 12:00
#BSUB  -o ds_result.out
#BSUB  -e ds_result.err

mpiexec -n 1 python distributed_search.py
mpiexec -n 2 python distributed_search.py
mpiexec -n 4 python distributed_search.py
mpiexec -n 8 python distributed_search.py
mpiexec -n 16 python distributed_search.py
mpiexec -n 32 python distributed_search.py
#mpiexec -n 40 python distributed_search.py
#mpiexec -n 64 python distributed_search.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   16541.00 sec.
    Max Memory :                                 11939 MB
    Average Memory :                             945.08 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2 MB
    Max Processes :                              37
    Max Threads :                                1613
    Run time :                                   5161 sec.
    Turnaround time :                            12820 sec.

The output (if any) follows:

==================================
          WORLD SIZE: 1          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 12155
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0308
Parallel Search Time: 2400.91780 seconds.
==================================
          WORLD SIZE: 2          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 12155
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0308
Parallel Search Time: 1286.13272 seconds.
==================================
          WORLD SIZE: 4          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 12155
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0308
Parallel Search Time: 677.85973 seconds.
==================================
          WORLD SIZE: 8          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 12155
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0308
Parallel Search Time: 381.38065 seconds.
==================================
          WORLD SIZE: 16          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 12155
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0308
Parallel Search Time: 200.74111 seconds.
==================================
          WORLD SIZE: 32          
==================================
Chosen method: Static Distribution  SCIPY_DH
Total model count: 12155
Fixed model: airplane_0627.
-----------------------------------------------
Closest model to airplane_0627 is airplane_0308
Parallel Search Time: 137.42282 seconds.


PS:

Read file <ds_result.err> for stderr output of this job.

